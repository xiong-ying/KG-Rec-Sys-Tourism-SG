{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb64c55",
   "metadata": {},
   "source": [
    "# Ensemble Learning - Majority Voting\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759796d",
   "metadata": {},
   "source": [
    "\n",
    "Ensemble Learning based on following recommendation algortithms:\n",
    "\n",
    "- Content Based Filtering - Heuristic\n",
    "\n",
    "- Content Based Filtering - Node Similarity\n",
    "\n",
    "- Collaborative Filtering - UserKnn with FastRP\n",
    "\n",
    "- Collaborative Filtering - ItemKnn with FastRP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4596ec8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Neo4j server with a recent version (2.0+) of GDS installed.\n",
    "\n",
    "The `graphdatascience` Python library to operate Neo4j GDS.\n",
    "\n",
    "`Cypher` query to generate recommendations.\n",
    "\n",
    "`py2neo` package to write pandas dataframe back to neo4j database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39328998",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Installing and importing  dependencies, and setting up neo4j python driver, py2neo and GDS client connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fde45a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphdatascience in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphdatascience) (1.11.2)\n",
      "Requirement already satisfied: neo4j<6.0,>=4.4.2 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphdatascience) (5.18.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphdatascience) (2.1.3)\n",
      "Requirement already satisfied: pyarrow<15.0,>=10.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphdatascience) (14.0.2)\n",
      "Requirement already satisfied: textdistance<5.0,>=4.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphdatascience) (4.6.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphdatascience) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphdatascience) (4.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphdatascience) (2.31.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from neo4j<6.0,>=4.4.2->graphdatascience) (2023.3.post1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3.0,>=1.0->graphdatascience) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3.0,>=1.0->graphdatascience) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3.0,>=1.0->graphdatascience) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0,>=4.0->graphdatascience) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->graphdatascience) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->graphdatascience) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->graphdatascience) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->graphdatascience) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->graphdatascience) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: py2neo in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2021.2.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2024.2.2)\n",
      "Requirement already satisfied: interchange~=2021.0.4 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2021.0.4)\n",
      "Requirement already satisfied: monotonic in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (1.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from py2neo) (23.2)\n",
      "Requirement already satisfied: pansi>=2020.7.3 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2020.7.3)\n",
      "Requirement already satisfied: pygments>=2.0.0 in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from py2neo) (2.16.1)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\xiong\\appdata\\roaming\\python\\python312\\site-packages (from py2neo) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2.2.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\xiong\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from interchange~=2021.0.4->py2neo) (2023.3.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary dependencies\n",
    "%pip install graphdatascience\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install py2neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44abe56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import configparser\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from py2neo import Graph\n",
    "from neo4j import GraphDatabase\n",
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1c57b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom database properties \n",
      "HOST: bolt://3.94.20.148:7687; DATABASE: neo4j; PASSWORD: anthems-schedulers-blade\n"
     ]
    }
   ],
   "source": [
    "# Using an ini file for credentials, otherwise providing defaults\n",
    "HOST = 'neo4j://localhost'\n",
    "DATABASE = 'neo4j'\n",
    "PASSWORD = 'password'\n",
    "\n",
    "NEO4J_CONF_FILE = 'neo4j.ini'\n",
    "\n",
    "if NEO4J_CONF_FILE is not None and os.path.exists(NEO4J_CONF_FILE):\n",
    "    config = configparser.RawConfigParser()\n",
    "    config.read(NEO4J_CONF_FILE)\n",
    "    HOST = config['NEO4J']['HOST']\n",
    "    DATABASE = config['NEO4J']['DATABASE']\n",
    "    PASSWORD = config['NEO4J']['PASSWORD']\n",
    "    print(f'Using custom database properties \\nHOST: {HOST}; DATABASE: {DATABASE}; PASSWORD: {PASSWORD}')\n",
    "else:\n",
    "    print('Could not find database properties file, using defaults')\n",
    "\n",
    "# Connecting with neo4j python driver\n",
    "driver = GraphDatabase.driver(HOST, auth=(DATABASE, PASSWORD))\n",
    "\n",
    "# Connecting with the Neo4j database using GDS library\n",
    "gds = GraphDataScience(HOST,auth=(DATABASE, PASSWORD))\n",
    "\n",
    "# Connect to Neo4j database using py2neo\n",
    "graph = Graph(HOST, auth=(DATABASE, PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51a1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver helper function\n",
    "def run(driver, query, params=None):\n",
    "    with driver.session() as session:\n",
    "        if params is not None:\n",
    "            return [r for r in session.run(query, params)]\n",
    "        else:\n",
    "            return [r for r in session.run(query)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712231a",
   "metadata": {},
   "source": [
    "## 1) Content Based Filtering Recommendations - Heuristic Method\n",
    "\n",
    "### query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e511aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: make recommendation based on Content Based Filtering Recommendations - Heuristic Method\n",
    "# INPUT: user_id, poi_id\n",
    "# OUTPUT: dataframe[user_id, poi_id, rec_poi_id]\n",
    "\n",
    "def heuristic_recommendation(user_id, poi_id):\n",
    "    # get pois in the same region as reviewed_poi by the user\n",
    "    records_region = run(driver, textwrap.dedent(\"\"\"\\\n",
    "        MATCH (user {id: $user_id})-[:REVIEWED]->(poi:Poi {id: $poi_id})-[:LOCATED_AT]->(region:Region)<-[:LOCATED_AT]-(other_poi:Poi)<-[rated:RATED]-(review:Review)\n",
    "        WHERE poi <> other_poi\n",
    "        WITH user, poi, other_poi, region, count(DISTINCT rated) AS num_reviews\n",
    "        RETURN user.id AS user_id, poi.id AS poi_id, other_poi.id AS rec_poi_id, region.name AS region, num_reviews AS occurrences\n",
    "        \"\"\"),\n",
    "        params = {'user_id': user_id, 'poi_id': poi_id}\n",
    "    )\n",
    "\n",
    "    # get pois in the same category as reviewed_poi by the user \n",
    "    records_category = run(driver, textwrap.dedent(\"\"\"\\\n",
    "        MATCH (user {id: $user_id})-[:REVIEWED]->(poi:Poi {id: $poi_id})-[:BELONGS_TO]->(category:Category)<-[:BELONGS_TO]-(other_poi:Poi)<-[rated:RATED]-(review:Review)\n",
    "        WHERE poi <> other_poi\n",
    "        WITH user, poi, other_poi, category, count(DISTINCT rated) AS num_reviews\n",
    "        RETURN user.id AS user_id, poi.id AS poi_id, other_poi.id AS rec_poi_id, category.name AS category_name, num_reviews AS occurrences\n",
    "        \"\"\"),\n",
    "        params = {'user_id': user_id, 'poi_id': poi_id}\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Convert the result to a DataFrame\n",
    "    if records_region:\n",
    "        df_records_region = pd.DataFrame([dict(record) for record in records_region])\n",
    "        # Group by 'poi_id', 'poi_name', and 'occurrences', then aggregate the count of occurrences\n",
    "        df_records_region_agg = df_records_region.groupby(['user_id', 'poi_id', 'rec_poi_id', 'occurrences']).size().reset_index(name='weight')\n",
    "    else:\n",
    "        df_records_region_agg = pd.DataFrame(columns=['user_id', 'poi_id', 'rec_poi_id', 'occurrences', 'weight'])\n",
    "\n",
    "    if records_category:\n",
    "        df_records_category = pd.DataFrame([dict(record) for record in records_category])\n",
    "        # Group by 'poi_id', 'poi_name', and 'occurrences', then aggregate the count of occurrences\n",
    "        df_records_category_agg = df_records_category.groupby(['user_id', 'poi_id', 'rec_poi_id', 'occurrences']).size().reset_index(name='weight')\n",
    "    else:\n",
    "       df_records_category_agg = pd.DataFrame(columns=['user_id', 'poi_id', 'rec_poi_id', 'occurrences', 'weight'])\n",
    "\n",
    "\n",
    "    # compute appearance fequency of pois in both lists\n",
    "\n",
    "    # Merge the two DataFrames on 'rec_poi_id'\n",
    "    recommended_interactions = pd.merge(df_records_region_agg, df_records_category_agg, on='rec_poi_id', suffixes=('_region', '_category'), how='outer')\n",
    "\n",
    "    # Fill NaN values in '_region' columns with values from '_category' columns\n",
    "    recommended_interactions['user_id_region'].fillna(recommended_interactions['user_id_category'], inplace=True)\n",
    "    recommended_interactions['poi_id_region'].fillna(recommended_interactions['poi_id_category'], inplace=True)\n",
    "    recommended_interactions['occurrences_region'].fillna(recommended_interactions['occurrences_category'], inplace=True)\n",
    "\n",
    "    # Rename the columns '_region'\n",
    "    recommended_interactions.rename(columns={'user_id_region': 'user_id'}, inplace=True)\n",
    "    recommended_interactions.rename(columns={'poi_id_region': 'poi_id'}, inplace=True)\n",
    "    recommended_interactions.rename(columns={'occurrences_region': 'occurrences'}, inplace=True)\n",
    "\n",
    "    # Fill NaN values with 0 for the 'weight' columns\n",
    "    recommended_interactions['weight_region'].fillna(0, inplace=True)\n",
    "    recommended_interactions['weight_category'].fillna(0, inplace=True)\n",
    "    # Sum the 'weight' columns to get the total weight\n",
    "    recommended_interactions['total_weight'] = recommended_interactions['weight_region'] + recommended_interactions['weight_category']\n",
    "\n",
    "    # Drop the individual 'weight' columns if needed\n",
    "    recommended_interactions.drop(['user_id_category', 'poi_id_category', 'occurrences_category', 'weight_region', 'weight_category'], axis=1, inplace=True)\n",
    "    # Order the DataFrame by 'total_weight' in descending order, then by 'occurrences'\n",
    "    recommended_interactions = recommended_interactions.sort_values(by=['total_weight', 'occurrences'], ascending=[False, False])\n",
    "    # Reindex the DataFrame\n",
    "    recommended_interactions.reset_index(drop=True, inplace=True)\n",
    "    # Rearrange the columns\n",
    "    recommended_interactions = recommended_interactions[['user_id', 'poi_id', 'rec_poi_id']]\n",
    "    # drop duplicate\n",
    "    recommended_interactions = recommended_interactions.drop_duplicates()\n",
    "\n",
    "    # Display the merged DataFrame\n",
    "    return recommended_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8199d5",
   "metadata": {},
   "source": [
    "## 2) Content-based Filtering Recommendations - Node Similarity\n",
    "\n",
    "### preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1992fa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\1980835958.py:112: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_textual_cols.reset_index(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# duration: 6m\n",
    "\n",
    "# Extract raw data of node poi and its attributes from GDS\n",
    "result = gds.run_cypher(\"\"\"\n",
    "MATCH (poi:Poi)\n",
    "OPTIONAL MATCH (poi)-[:BELONGS_TO]->(category:Category)\n",
    "OPTIONAL MATCH (poi)-[:LOCATED_AT]->(region:Region)\n",
    "RETURN poi.id AS poi_id, \n",
    "       poi.name AS name, \n",
    "                        \n",
    "       poi.description AS description, \n",
    "\n",
    "       poi.openingHours AS opening_hours, \n",
    "       poi.duration AS duration, \n",
    "       category.name AS category, \n",
    "       region.name AS region,\n",
    "                        \n",
    "       poi.price AS price, \n",
    "       poi.avgRating AS avg_rating, \n",
    "       poi.numReviews AS num_reviews, \n",
    "       poi.numReviews_5 AS num_reviews_5, \n",
    "       poi.numReviews_4 AS num_reviews_4, \n",
    "       poi.numReviews_3 AS num_reviews_3, \n",
    "       poi.numReviews_2 AS num_reviews_2, \n",
    "       poi.numReviews_1 AS num_reviews_1\n",
    "\"\"\")\n",
    "\n",
    "# Convert result to DataFrame\n",
    "df_pois = pd.DataFrame(result)\n",
    "\n",
    "# Extracting distinct poi_id and poi_name\n",
    "df_distinct_pois = df_pois.copy()\n",
    "df_distinct_pois = df_distinct_pois[['poi_id', 'name']].drop_duplicates()\n",
    "\n",
    "# Numerical Features - Min-Max Normalization\n",
    "# Attributes: 'price', 'avg_rating', 'num_reviews', 'num_reviews_5', 'num_reviews_4', 'num_reviews_3', 'num_reviews_2', 'num_reviews_1'\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "numerical_cols = ['price', 'avg_rating', 'num_reviews', 'num_reviews_5', 'num_reviews_4', 'num_reviews_3', 'num_reviews_2', 'num_reviews_1']\n",
    "# Create a new DataFrame with scaled columns and poi_id\n",
    "df_numerical_cols = df_pois.copy()\n",
    "df_numerical_cols = df_numerical_cols[['poi_id'] + numerical_cols]\n",
    "# retaining only distinct entries\n",
    "df_numerical_cols = df_numerical_cols.drop_duplicates()\n",
    "# Fill missing values in numerical columns with 0\n",
    "df_numerical_cols.fillna(0, inplace=True)\n",
    "\n",
    "# scale properties\n",
    "df_numerical_cols[numerical_cols] = scaler.fit_transform(df_numerical_cols[numerical_cols])\n",
    "\n",
    "\n",
    "# Categorical Features - One-hot Encoding\n",
    "# Attributes: category, region, opening_Hours, duration\n",
    "\n",
    "categorical_cols = ['category', 'region', 'opening_hours', 'duration']\n",
    "\n",
    "# Copy df_pois with only the specified categorical columns\n",
    "df_categorical_cols = df_pois.copy()\n",
    "df_categorical_cols = df_categorical_cols[['poi_id'] + categorical_cols]\n",
    "\n",
    "# Do one-hot encoding for categorical columns\n",
    "df_categorical_cols = pd.get_dummies(df_categorical_cols, columns=categorical_cols)\n",
    "\n",
    "\n",
    "# Merge rows with the same poi_id while applying OR logical operation\n",
    "df_categorical_cols = df_categorical_cols.groupby('poi_id').max().reset_index()\n",
    "\n",
    "\n",
    "# Textual Features - Token count\n",
    "# Attributes: description\n",
    "\n",
    "textual_cols = ['description']\n",
    "\n",
    "# Copy df_pois with only the specified textual columns\n",
    "df_cols = df_pois.copy()\n",
    "df_cols = df_cols[['poi_id'] + textual_cols]\n",
    "\n",
    "# retaining only distinct entries\n",
    "df_cols = df_cols.drop_duplicates()\n",
    "\n",
    "# create a mask to check if description column contains empty strings\n",
    "empty_description = df_cols['description'] == ''\n",
    "# Fill empty strings with \"NULL\"\n",
    "df_cols.loc[empty_description, 'description'] = 'NULL'\n",
    "\n",
    "# initialize token counter, ignore stop words\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Create an empty DataFrame to store token counts\n",
    "df_textual_cols = pd.DataFrame()\n",
    "\n",
    "# Iterate over each POI and its description\n",
    "for index, row in df_cols.iterrows():\n",
    "\n",
    "    # Tokenize the description\n",
    "    description = [row['description']]\n",
    "    # print(f'description: {description}')\n",
    "    \n",
    "    # Count token and store in sparse matrix, then convert to dense matrix\n",
    "    sparse_matrix = count_vectorizer.fit_transform(description)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    \n",
    "    # Create DataFrame from the dense matrix\n",
    "    df_token_counts = pd.DataFrame(\n",
    "        doc_term_matrix,\n",
    "        columns=count_vectorizer.get_feature_names_out(),\n",
    "        index=[row['poi_id']]\n",
    "    )\n",
    "    \n",
    "    # Append the DataFrame to df_token_counts\n",
    "    df_textual_cols = pd.concat([df_textual_cols, df_token_counts])\n",
    "\n",
    "# Reset index, rename to poi_id, and fill NaN values with 0\n",
    "df_textual_cols.reset_index(inplace=True)\n",
    "df_textual_cols = df_textual_cols.rename(columns={'index': 'poi_id'})\n",
    "df_textual_cols.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Compute pair-wise similarity between pois\n",
    "\n",
    "# Initialize similarity matrix\n",
    "similarity_matrix = {}\n",
    "\n",
    "# Calculate similarity for each pair of distinct POIs\n",
    "for i in range(len(df_distinct_pois)):\n",
    "    for j in range(i+1, len(df_distinct_pois)):\n",
    "\n",
    "        # get poi id in pairs\n",
    "\n",
    "        poi1_id, poi2_id = df_distinct_pois.iloc[i]['poi_id'], df_distinct_pois.iloc[j]['poi_id']\n",
    "\n",
    "        # Calculate Jaccard similarity for categorical attributes\n",
    "\n",
    "        poi1_categorical_row = df_categorical_cols[df_categorical_cols['poi_id'] == poi1_id].iloc[:, 1:].values.flatten()\n",
    "        poi2_categorical_row = df_categorical_cols[df_categorical_cols['poi_id'] == poi2_id].iloc[:, 1:].values.flatten()\n",
    "        cat_cols_similarity = jaccard_score(poi1_categorical_row, poi2_categorical_row)\n",
    "\n",
    "        # Calculate euclidean distance similarity for numerical attributes\n",
    "\n",
    "        poi1_numerical_row = df_numerical_cols[df_numerical_cols['poi_id'] == poi1_id].iloc[:, 1:].values.flatten()\n",
    "        poi2_numerical_row = df_numerical_cols[df_numerical_cols['poi_id'] == poi2_id].iloc[:, 1:].values.flatten()\n",
    "        euclidean_distance = math.dist(poi1_numerical_row, poi2_numerical_row)\n",
    "        num_cols_similarity = 1 / ( 1 + euclidean_distance )\n",
    "\n",
    "        # Calculate cosine similarity for textual attributes\n",
    "\n",
    "        poi1_textual_row = df_textual_cols[df_textual_cols['poi_id'] == poi1_id].iloc[:, 1:].values.flatten()\n",
    "        poi2_textual_row = df_textual_cols[df_textual_cols['poi_id'] == poi2_id].iloc[:, 1:].values.flatten()\n",
    "        text_cols_similarity = cosine_similarity([poi1_textual_row, poi2_textual_row])[0][1]\n",
    "\n",
    "        # Compute weighted overall similarity\n",
    "\n",
    "        num_cat_cols = len(categorical_cols)\n",
    "        num_num_cols = len(numerical_cols)\n",
    "        num_text_cols = len(textual_cols)\n",
    "        similarity = ( num_cat_cols * cat_cols_similarity + num_num_cols * num_cols_similarity + num_text_cols * text_cols_similarity ) / ( num_cat_cols + num_num_cols + num_text_cols )\n",
    "        \n",
    "        # Store similarity in the matrix\n",
    "        similarity_matrix[(poi1_id, poi2_id)] = similarity\n",
    "\n",
    "# Convert similarity matrix to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_matrix.items(), columns=['POI Pair', 'Similarity'])\n",
    "# Drop rows where Similarity is less than 0.5\n",
    "df_similarity = df_similarity[df_similarity['Similarity'] >= 0.5]\n",
    "# Split the 'POI Pair' column into two separate columns\n",
    "df_similarity[['poi1_id', 'poi2_id']] = pd.DataFrame(df_similarity['POI Pair'].tolist(), index=df_similarity.index)\n",
    "# Drop the original 'POI Pair' column\n",
    "df_similarity.drop(columns=['POI Pair'], inplace=True)\n",
    "# Reorder the columns\n",
    "df_similarity = df_similarity[['poi1_id', 'poi2_id', 'Similarity']]\n",
    "# Reorder the DataFrame by the column \"Similarity\"\n",
    "df_similarity = df_similarity.sort_values(by='Similarity', ascending=False)\n",
    "# Reindex the DataFrame\n",
    "df_similarity = df_similarity.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# write SIMILAR relationship between pois with property similarity\n",
    "# duration: 5m\n",
    "\n",
    "# Iterate over the DataFrame rows and write the relationships to Neo4j\n",
    "for index, row in df_similarity.iterrows():\n",
    "    poi1_id = row['poi1_id']\n",
    "    poi2_id = row['poi2_id']\n",
    "    similarity = row['Similarity']\n",
    "    \n",
    "    # Write undirected relationship between poi1_id and poi2_id with similarity property\n",
    "    query = f\"\"\"\n",
    "    MATCH (poi1:Poi {{id: {poi1_id}}})\n",
    "    MATCH (poi2:Poi {{id: {poi2_id}}})\n",
    "    MERGE (poi1)-[s1:CBF_SIMILAR]->(poi2)\n",
    "    ON CREATE SET s1.score = {similarity}\n",
    "    MERGE (poi1)<-[s2:CBF_SIMILAR]-(poi2)\n",
    "    ON CREATE SET s2.score = {similarity}\n",
    "    \"\"\"\n",
    "    graph.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1e7af",
   "metadata": {},
   "source": [
    "### query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c874077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: make recommendation based on Content Based Filtering Recommendations - Node Similarity\n",
    "# INPUT: poi_id\n",
    "# OUTPUT: dataframe[poi_id, rec_poi_id]\n",
    "\n",
    "def similar_poi_recommendation(poi_id):\n",
    "    result = gds.run_cypher(\n",
    "        \"\"\"\n",
    "            MATCH (p1:Poi {id: $target_poi})-[s:CBF_SIMILAR]->(p2:Poi)\n",
    "            RETURN p1.id as poi_id, p2.id as rec_poi_id\n",
    "            ORDER BY s.score DESC\n",
    "        \"\"\", params = {'target_poi': poi_id}\n",
    "    )\n",
    "    result = result.drop_duplicates()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156d150",
   "metadata": {},
   "source": [
    "## 3) Collaborative Filtering Recommendations - User-Based kNN based on FastRP embeddings\n",
    "\n",
    "### preparation\n",
    "\n",
    "These projected graph and fastRP embedding will be used for both algorithm (3）and (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "534680a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embedding vectors produced: 58725\n"
     ]
    }
   ],
   "source": [
    "# Projection Graph\n",
    "\n",
    "# duration: 20s\n",
    "\n",
    "# define how to project database into GDS\n",
    "node_projection = [\"User\", \"Poi\"]\n",
    "relationship_projection = {\"REVIEWED\": {\"orientation\": \"UNDIRECTED\", \"properties\": \"rating\"}}\n",
    "\n",
    "# proceed with projection\n",
    "G, result = gds.graph.project(\"myGraph\", node_projection, relationship_projection)\n",
    "\n",
    "\n",
    "# Create Fast RP embeddings\n",
    "\n",
    "# run FastRP and mutate our projected graph with the results\n",
    "result = gds.fastRP.mutate(\n",
    "    G,\n",
    "    randomSeed=42,\n",
    "    embeddingDimension=256,\n",
    "    relationshipWeightProperty=\"rating\",\n",
    "    iterationWeights=[0, 1, 1, 1],\n",
    "    mutateProperty=\"embedding\"\n",
    ")\n",
    "\n",
    "print(f\"Number of embedding vectors produced: {result['nodePropertiesWritten']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169d3af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships produced: 703872\n",
      "Nodes compared: 58656\n",
      "Mean similarity: 0.9976186487680783\n"
     ]
    }
   ],
   "source": [
    "# Similarity with User-based KNN\n",
    "\n",
    "# Run the kNN with optimal topK hyperparameter and write back to db\n",
    "# duration: 3m\n",
    "\n",
    "topK_best = 12\n",
    "\n",
    "result = gds.knn.write(\n",
    "    G,\n",
    "    topK=topK_best,\n",
    "    nodeLabels=['User'],\n",
    "    nodeProperties=[\"embedding\"],\n",
    "    randomSeed=42,\n",
    "    concurrency=1,\n",
    "    sampleRate=1.0,\n",
    "    deltaThreshold=0.0,\n",
    "    writeRelationshipType=\"CF_SIMILAR_USER\",\n",
    "    writeProperty=\"score\",\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Relationships produced: {result['relationshipsWritten']}\")\n",
    "print(f\"Nodes compared: {result['nodesCompared']}\")\n",
    "print(f\"Mean similarity: {result['similarityDistribution']['mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699735d2",
   "metadata": {},
   "source": [
    "### query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f815b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: make recommendation based on Collaborative Filtering Recommendations - User-Based kNN based on FastRP embeddings\n",
    "# INPUT: user_id\n",
    "# OUTPUT: dataframe[user_id, rec_poi_id]\n",
    "\n",
    "def userKNN_recommendation(user_id):\n",
    "\n",
    "    result = gds.run_cypher(\n",
    "        \"\"\"\n",
    "            MATCH (u1:User {id: $target_user})-[s:CF_SIMILAR_USER]->(u2:User)-[:REVIEWED]->(p:Poi)\n",
    "            WITH u1, p, s.score AS user_similarity\n",
    "            RETURN u1.id as user_id, p.id as rec_poi_id\n",
    "            ORDER BY user_similarity DESC, p.avgRating DESC\n",
    "        \"\"\", params = {'target_user': user_id}\n",
    "    )\n",
    "    result = result.drop_duplicates()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774d899",
   "metadata": {},
   "source": [
    "## 4) Collaborative Filtering Recommendations - Item-Based kNN based on FastRP embeddings\n",
    "\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f26abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships produced: 112\n",
      "Nodes compared: 69\n",
      "Mean similarity: 0.6929127488817487\n"
     ]
    }
   ],
   "source": [
    "# Similarity with Item-based KNN\n",
    "\n",
    "# Run the kNN with optimal topK hyperparameter and write back to db\n",
    "\n",
    "topK_best = 2\n",
    "\n",
    "result = gds.knn.write(\n",
    "    G,\n",
    "    topK=topK_best,\n",
    "    nodeLabels = ['Poi'],\n",
    "    nodeProperties=[\"embedding\"],\n",
    "    randomSeed=42,\n",
    "    concurrency=1,\n",
    "    sampleRate=1.0,\n",
    "    deltaThreshold=0.0,\n",
    "    similarityCutoff = 0.5,\n",
    "    writeRelationshipType=\"CF_SIMILAR_POI\",\n",
    "    writeProperty=\"score\"\n",
    ")\n",
    "\n",
    "print(f\"Relationships produced: {result['relationshipsWritten']}\")\n",
    "print(f\"Nodes compared: {result['nodesCompared']}\")\n",
    "print(f\"Mean similarity: {result['similarityDistribution']['mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977aed8e",
   "metadata": {},
   "source": [
    "### Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c93e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: make recommendation based on Collaborative Filtering Recommendations - Item-Based kNN based on FastRP embeddings\n",
    "# INPUT: poi_id\n",
    "# OUTPUT: dataframe[poi_id, rec_poi_id]\n",
    "\n",
    "def itemKNN_recommendation(poi_id):\n",
    "    result = gds.run_cypher(\n",
    "        \"\"\"\n",
    "            MATCH (p1:Poi {id: $target_poi})-[s:CF_SIMILAR_POI]->(p2:Poi)\n",
    "            RETURN p1.id as poi_id, p2.id as rec_poi_id\n",
    "            ORDER BY s.score DESC, p2.avgRating DESC\n",
    "        \"\"\", params = {'target_poi': poi_id}\n",
    "    )\n",
    "    result = result.drop_duplicates()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bf299",
   "metadata": {},
   "source": [
    "## Making recommendations with Ensemble Learning\n",
    "\n",
    "Make poi recommendations for user from other similar users using a simple Cypher query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14bb1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: helper to cleaning up each df after calling recommendation algorithm, prepare them for ensemble learning\n",
    "def df_cleaning (df):\n",
    "    if not df.empty:    # Reset index, get rank, and re-arrange columns\n",
    "        df.reset_index(drop=True, inplace=True)          \n",
    "        df = df.reset_index().rename(columns={'index': 'rank'})\n",
    "        df['rank'] += 1\n",
    "        df = df.reindex(columns=['user_id', 'poi_id', 'rec_poi_id', 'rank', 'df_name'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316862cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>rec_poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17518</td>\n",
       "      <td>310900</td>\n",
       "      <td>4400781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17518</td>\n",
       "      <td>310900</td>\n",
       "      <td>591382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17518</td>\n",
       "      <td>310900</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  poi_id  rec_poi_id\n",
       "0    17518  310900     4400781\n",
       "1    17518  310900      591382\n",
       "2    17518  310900     2149128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUNCTION: make recommendation based on Ensemble Learning - Majority Voting\n",
    "# INPUT: poi_id, user_id, algo_combination\n",
    "# OUTPUT: dataframe[poi_id, user_id, rec_poi_id]\n",
    "\n",
    "def ensemble_recommendation(poi_id, user_id, algo_combination):\n",
    "\n",
    "    # Based on the chosen algorithm combination, decide whether to call each recommendation function\n",
    "\n",
    "    if 1 in algo_combination:\n",
    "        rec_CBF_heuristic = heuristic_recommendation(user_id, poi_id)   # OUTPUT: dataframe[user_id, poi_id, rec_poi_id]\n",
    "        rec_CBF_heuristic['df_name'] = 'rec_CBF_heuristic'              # Add DataFrame name as a column\n",
    "        rec_CBF_heuristic = df_cleaning (rec_CBF_heuristic)             # Clean up df for ensemble\n",
    "    else:\n",
    "        rec_CBF_heuristic = pd.DataFrame()\n",
    "\n",
    "    if 2 in algo_combination:\n",
    "        rec_CBF_similarity = similar_poi_recommendation(poi_id)         # OUTPUT: dataframe[poi_id, rec_poi_id]\n",
    "        rec_CBF_similarity['df_name'] = 'rec_CBF_similarity'            # Add DataFrame name as a column\n",
    "        rec_CBF_similarity['user_id'] = user_id                         # Add missing columns\n",
    "        rec_CBF_similarity = df_cleaning (rec_CBF_similarity)           # Clean up df for ensemble\n",
    "    else:\n",
    "        rec_CBF_similarity = pd.DataFrame()\n",
    "\n",
    "    if 3 in algo_combination:\n",
    "        rec_CF_userKnn =  userKNN_recommendation(user_id)               # OUTPUT: dataframe[user_id, rec_poi_id]\n",
    "        rec_CF_userKnn['df_name'] = 'rec_CF_userKnn'                    # Add DataFrame name as a column\n",
    "        rec_CF_userKnn['poi_id'] = poi_id                               # Add missing columns\n",
    "        rec_CF_userKnn = df_cleaning (rec_CF_userKnn)                   # Clean up df for ensemble\n",
    "    else:\n",
    "        rec_CF_userKnn = pd.DataFrame()\n",
    "\n",
    "    if 4 in algo_combination:\n",
    "        rec_CF_itemKnn = itemKNN_recommendation(poi_id)                 # OUTPUT: dataframe[poi_id, rec_poi_id]\n",
    "        rec_CF_itemKnn['df_name'] = 'rec_CF_itemKnn'                    # Add DataFrame name as a column\n",
    "        rec_CF_itemKnn['user_id'] = user_id                             # Add missing columns\n",
    "        rec_CF_itemKnn = df_cleaning (rec_CF_itemKnn)                   # Clean up df for ensemble\n",
    "    else:\n",
    "        rec_CF_itemKnn = pd.DataFrame()\n",
    "\n",
    "    # Print the reordered DataFrames\n",
    "    #print(rec_CBF_heuristic)\n",
    "    #print(rec_CBF_similarity)\n",
    "    #print(rec_CF_userKnn)\n",
    "    #print(rec_CF_itemKnn)\n",
    "    \n",
    "    # Concatenate the DataFrames along the rows\n",
    "    merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
    "    #print(f'merged_df: \\n{merged_df}')\n",
    "\n",
    "    # check if merged df is not empty\n",
    "    if not merged_df.empty:\n",
    "        # Group by user_id, poi_id, rec_poi_id and compute average rank and count\n",
    "        grouped_df = merged_df.groupby(['user_id', 'poi_id', 'rec_poi_id']).agg({'rank': 'mean', 'df_name': 'count'}).reset_index()\n",
    "\n",
    "        # Rename the count column to count\n",
    "        grouped_df.rename(columns={'df_name': 'count'}, inplace=True)\n",
    "\n",
    "        # drop any item with count = 1\n",
    "        grouped_df = grouped_df[grouped_df['count'] > 1]\n",
    "\n",
    "        # Sort by count in descending order and average rank in ascending order\n",
    "        sorted_df = grouped_df.sort_values(by=['count', 'rank'], ascending=[False, True])\n",
    "        #print(f'sorted_df: \\n{sorted_df}')\n",
    "\n",
    "        # Drop the 'count' and 'rank' columns\n",
    "        result = sorted_df.drop(columns=['count', 'rank'])\n",
    "        #result = sorted_df.copy()\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        result = merged_df.drop(columns=['df_name'])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# target user's id\n",
    "user_id = 17518\n",
    "# target poi's id\n",
    "poi_id = 310900\n",
    "\n",
    "# combination of algorithm for ensemble learning\n",
    "algo_combination = [1,2,3,4]\n",
    "\n",
    "#Choose from the below Algorithms:\n",
    "#(1) Content Based Filtering - Heuristic\n",
    "#(2) Content Based Filtering - Node Similarity\n",
    "#(3) Collaborative Filtering - UserKnn with FastRP\n",
    "#(4) Collaborative Filtering - ItemKnn with FastRP\n",
    "\n",
    "ensemble_recommendation(poi_id, user_id, algo_combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a339b19",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "154fd9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi.id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4400781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>678639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>17821111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>17738872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>26356283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>21353012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>20318911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      poi.id\n",
       "0    2149128\n",
       "1     310900\n",
       "2    4400781\n",
       "3     324542\n",
       "4     678639\n",
       "..       ...\n",
       "64  17821111\n",
       "65  17738872\n",
       "66  26356283\n",
       "67  21353012\n",
       "68  20318911\n",
       "\n",
       "[69 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframes of pois\n",
    "df_pois = gds.run_cypher(\"\"\"\\\n",
    "    MATCH (poi:Poi)    \n",
    "    RETURN poi.id\n",
    "    \"\"\")\n",
    "\n",
    "df_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2d25de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>847</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21070</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21061</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21003</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21227</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85029</th>\n",
       "      <td>58650</td>\n",
       "      <td>7275891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85030</th>\n",
       "      <td>58652</td>\n",
       "      <td>7275891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85031</th>\n",
       "      <td>58654</td>\n",
       "      <td>17821111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85032</th>\n",
       "      <td>58656</td>\n",
       "      <td>17821111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85033</th>\n",
       "      <td>58655</td>\n",
       "      <td>17821111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85034 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id    poi_id\n",
       "0          847   2149128\n",
       "1        21070   2149128\n",
       "2        21061   2149128\n",
       "3        21003   2149128\n",
       "4        21227   2149128\n",
       "...        ...       ...\n",
       "85029    58650   7275891\n",
       "85030    58652   7275891\n",
       "85031    58654  17821111\n",
       "85032    58656  17821111\n",
       "85033    58655  17821111\n",
       "\n",
       "[85034 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframes of reviews\n",
    "# duration: 32s\n",
    "\n",
    "df_reviews = gds.run_cypher(\"\"\"\\\n",
    "    MATCH (user:User)-[review:REVIEWED]->(poi:Poi)\n",
    "    RETURN user.id AS user_id, poi.id AS poi_id\n",
    "    \"\"\")\n",
    "\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5965d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>20980</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>20419</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>20445</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>20803</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>20108</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84680</th>\n",
       "      <td>1753</td>\n",
       "      <td>1888873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84685</th>\n",
       "      <td>39201</td>\n",
       "      <td>1888873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84688</th>\n",
       "      <td>6967</td>\n",
       "      <td>1888873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84702</th>\n",
       "      <td>21691</td>\n",
       "      <td>1888873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85028</th>\n",
       "      <td>20445</td>\n",
       "      <td>7275891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4593 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   poi_id\n",
       "277      20980  2149128\n",
       "329      20419  2149128\n",
       "644      20445  2149128\n",
       "712      20803  2149128\n",
       "764      20108  2149128\n",
       "...        ...      ...\n",
       "84680     1753  1888873\n",
       "84685    39201  1888873\n",
       "84688     6967  1888873\n",
       "84702    21691  1888873\n",
       "85028    20445  7275891\n",
       "\n",
       "[4593 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by 'user_id' and count occurrences\n",
    "user_counts = df_reviews.groupby('user_id').size()\n",
    "\n",
    "# Filter out users with less than 5 occurrences\n",
    "valid_users = user_counts[user_counts >= 5].index\n",
    "\n",
    "# Filter the original DataFrame based on valid users\n",
    "filtered_df_reviews = df_reviews[df_reviews['user_id'].isin(valid_users)].copy()\n",
    "filtered_df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55edb06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43559</th>\n",
       "      <td>14882</td>\n",
       "      <td>678639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52599</th>\n",
       "      <td>17393</td>\n",
       "      <td>8634325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78604</th>\n",
       "      <td>25812</td>\n",
       "      <td>315470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81971</th>\n",
       "      <td>8079</td>\n",
       "      <td>13078277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66625</th>\n",
       "      <td>13223</td>\n",
       "      <td>8016698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80529</th>\n",
       "      <td>29227</td>\n",
       "      <td>310896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47967</th>\n",
       "      <td>29380</td>\n",
       "      <td>1837767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14022</th>\n",
       "      <td>7053</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>16889</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78186</th>\n",
       "      <td>21479</td>\n",
       "      <td>8016698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4133 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id    poi_id\n",
       "43559    14882    678639\n",
       "52599    17393   8634325\n",
       "78604    25812    315470\n",
       "81971     8079  13078277\n",
       "66625    13223   8016698\n",
       "...        ...       ...\n",
       "80529    29227    310896\n",
       "47967    29380   1837767\n",
       "14022     7053   2149128\n",
       "4555     16889   2149128\n",
       "78186    21479   8016698\n",
       "\n",
       "[4133 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into 90% training and 10% test sets\n",
    "df_train, df_test = train_test_split(filtered_df_reviews, test_size=0.1, random_state=100)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94716758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14227</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48823</th>\n",
       "      <td>23137</td>\n",
       "      <td>1837767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59373</th>\n",
       "      <td>41985</td>\n",
       "      <td>1888876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12031</th>\n",
       "      <td>9481</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81597</th>\n",
       "      <td>11220</td>\n",
       "      <td>2138910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44908</th>\n",
       "      <td>4635</td>\n",
       "      <td>678639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82916</th>\n",
       "      <td>38946</td>\n",
       "      <td>2139492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57328</th>\n",
       "      <td>27424</td>\n",
       "      <td>644919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53084</th>\n",
       "      <td>41524</td>\n",
       "      <td>317415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60426</th>\n",
       "      <td>32830</td>\n",
       "      <td>1888876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   poi_id\n",
       "14227     7435  2149128\n",
       "48823    23137  1837767\n",
       "59373    41985  1888876\n",
       "12031     9481  2149128\n",
       "81597    11220  2138910\n",
       "...        ...      ...\n",
       "44908     4635   678639\n",
       "82916    38946  2139492\n",
       "57328    27424   644919\n",
       "53084    41524   317415\n",
       "60426    32830  1888876\n",
       "\n",
       "[460 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f48f588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>rec_poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2149128</td>\n",
       "      <td>315470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>315470</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2149128</td>\n",
       "      <td>1837767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1837767</td>\n",
       "      <td>2149128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2149128</td>\n",
       "      <td>644919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83163</th>\n",
       "      <td>57330</td>\n",
       "      <td>14904083</td>\n",
       "      <td>8178306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83164</th>\n",
       "      <td>57915</td>\n",
       "      <td>8178306</td>\n",
       "      <td>3915753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83165</th>\n",
       "      <td>57915</td>\n",
       "      <td>3915753</td>\n",
       "      <td>8178306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83166</th>\n",
       "      <td>58217</td>\n",
       "      <td>317421</td>\n",
       "      <td>1888873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83167</th>\n",
       "      <td>58217</td>\n",
       "      <td>1888873</td>\n",
       "      <td>317421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id    poi_id rec_poi_id\n",
       "0           5   2149128     315470\n",
       "1           5    315470    2149128\n",
       "2           8   2149128    1837767\n",
       "3           8   1837767    2149128\n",
       "4          11   2149128     644919\n",
       "...       ...       ...        ...\n",
       "83163   57330  14904083    8178306\n",
       "83164   57915   8178306    3915753\n",
       "83165   57915   3915753    8178306\n",
       "83166   58217    317421    1888873\n",
       "83167   58217   1888873     317421\n",
       "\n",
       "[83168 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the true interactions of all poi-poi pair reviewed by distinct user\n",
    "# duration: 1m 30s\n",
    "\n",
    "# Group by user_id and aggregate poi_id as a list\n",
    "grouped = df_reviews.groupby('user_id')['poi_id'].apply(list)\n",
    "\n",
    "# Initialize an empty DataFrame for the result\n",
    "df_true_interactions = pd.DataFrame(columns=['user_id', 'poi_id', 'rec_poi_id'])\n",
    "\n",
    "# Iterate through each group\n",
    "for user_id, poi_ids in grouped.items():\n",
    "    # Create pairs of target_poi_id and poi_id for each user\n",
    "    entry = [(user_id, poi_id, other_poi_id) for poi_id in poi_ids for other_poi_id in poi_ids if poi_id != other_poi_id]\n",
    "    df_entry = pd.DataFrame(entry, columns=['user_id', 'poi_id', 'rec_poi_id'])\n",
    "    # Add pairs to the result DataFrame\n",
    "    df_true_interactions = pd.concat([df_true_interactions, df_entry], ignore_index=True)\n",
    "\n",
    "df_true_interactions = df_true_interactions.drop_duplicates()\n",
    "# Display the result DataFrame\n",
    "df_true_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "879abe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>rec_poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "      <td>310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "      <td>4400781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "      <td>324542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "      <td>1837767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "      <td>317415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>41524</td>\n",
       "      <td>317415</td>\n",
       "      <td>310896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>32830</td>\n",
       "      <td>1888876</td>\n",
       "      <td>324542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>32830</td>\n",
       "      <td>1888876</td>\n",
       "      <td>8634325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>32830</td>\n",
       "      <td>1888876</td>\n",
       "      <td>8016698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>32830</td>\n",
       "      <td>1888876</td>\n",
       "      <td>315470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2285 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id   poi_id rec_poi_id\n",
       "0       7435  2149128     310900\n",
       "1       7435  2149128    4400781\n",
       "2       7435  2149128     324542\n",
       "3       7435  2149128    1837767\n",
       "4       7435  2149128     317415\n",
       "...      ...      ...        ...\n",
       "2280   41524   317415     310896\n",
       "2281   32830  1888876     324542\n",
       "2282   32830  1888876    8634325\n",
       "2283   32830  1888876    8016698\n",
       "2284   32830  1888876     315470\n",
       "\n",
       "[2285 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all relevant instances by merging the true interactions and test instance on user id and poi id\n",
    "df_all_relevant = pd.merge(df_test, df_true_interactions, on=['user_id', 'poi_id'], how='inner')\n",
    "df_all_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012c528",
   "metadata": {},
   "source": [
    "start running algorithm to retrieve recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4602fd",
   "metadata": {},
   "source": [
    "duration for 500 instances: \n",
    "\n",
    "algo [1,2,3,4]: 15m \n",
    "\n",
    "algo [2,3,4]: 9m\n",
    "\n",
    "algo [1,3,4]: 11m\n",
    "\n",
    "algo [1,2,4]: 11m\n",
    "\n",
    "algo [1,2,3]: 11m\n",
    "\n",
    "algo [1,2]: 9m\n",
    "\n",
    "algo [1,3]: 8m\n",
    "\n",
    "algo [1,4]: 8m\n",
    "\n",
    "algo [2,3]: 6m\n",
    "\n",
    "algo [2,4]: 7m\n",
    "\n",
    "algo [3,4]: 7m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20686482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n",
      "C:\\Users\\xiong\\AppData\\Local\\Temp\\ipykernel_28268\\4206945121.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([rec_CF_itemKnn, rec_CF_userKnn, rec_CBF_similarity, rec_CBF_heuristic])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>rec_poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7435.0</td>\n",
       "      <td>2149128.0</td>\n",
       "      <td>4400781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7435.0</td>\n",
       "      <td>2149128.0</td>\n",
       "      <td>1837767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7435.0</td>\n",
       "      <td>2149128.0</td>\n",
       "      <td>310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23137.0</td>\n",
       "      <td>1837767.0</td>\n",
       "      <td>8016698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23137.0</td>\n",
       "      <td>1837767.0</td>\n",
       "      <td>8634325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>41524.0</td>\n",
       "      <td>317415.0</td>\n",
       "      <td>3915753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>32830.0</td>\n",
       "      <td>1888876.0</td>\n",
       "      <td>8634325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>32830.0</td>\n",
       "      <td>1888876.0</td>\n",
       "      <td>315470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>32830.0</td>\n",
       "      <td>1888876.0</td>\n",
       "      <td>379351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>32830.0</td>\n",
       "      <td>1888876.0</td>\n",
       "      <td>2344513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1718 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id     poi_id  rec_poi_id\n",
       "0      7435.0  2149128.0     4400781\n",
       "1      7435.0  2149128.0     1837767\n",
       "2      7435.0  2149128.0      310900\n",
       "3     23137.0  1837767.0     8016698\n",
       "4     23137.0  1837767.0     8634325\n",
       "...       ...        ...         ...\n",
       "1713  41524.0   317415.0     3915753\n",
       "1714  32830.0  1888876.0     8634325\n",
       "1715  32830.0  1888876.0      315470\n",
       "1716  32830.0  1888876.0      379351\n",
       "1717  32830.0  1888876.0     2344513\n",
       "\n",
       "[1718 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve recommendation for row in test set\n",
    "\n",
    "# Tuning selection of algorithm combination to get better result\n",
    "algo_combination = [1,2,3,4]\n",
    "\n",
    "df_all_retrieved = pd.DataFrame()\n",
    "for index, row in df_test.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    poi_id = row['poi_id']\n",
    "    #print(f'\\nuser_id: {user_id}')\n",
    "    #print(f'poi_id: {poi_id}')\n",
    "\n",
    "    recommended_interactions = ensemble_recommendation(poi_id, user_id, algo_combination)\n",
    "\n",
    "    # Concatenate recommended_interactions with test_recommendations\n",
    "    df_all_retrieved = pd.concat([df_all_retrieved, recommended_interactions], ignore_index=True)\n",
    "\n",
    "# Drop the duplicates\n",
    "df_all_retrieved = df_all_retrieved.drop_duplicates()\n",
    "\n",
    "df_all_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de20a6f",
   "metadata": {},
   "source": [
    "Number of retrieved entries:\n",
    "\n",
    "algo [1,2,3,4]: 1718 \n",
    "\n",
    "algo [2,3,4]: 549 \n",
    "\n",
    "algo [1,3,4]: 1003 \n",
    "\n",
    "algo [1,2,4]: 822 \n",
    "\n",
    "algo [1,2,3]: 1514  \n",
    "\n",
    "algo [1,2]: 645 \n",
    "\n",
    "algo [1,3]: 829 \n",
    "\n",
    "algo [1,4]: 170\n",
    "\n",
    "algo [2,3]: 314\n",
    "\n",
    "algo [2,4]: 201 \n",
    "\n",
    "algo [3,4]: 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b8b3f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>rec_poi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "      <td>310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "      <td>4400781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7435</td>\n",
       "      <td>2149128</td>\n",
       "      <td>1837767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23137</td>\n",
       "      <td>1837767</td>\n",
       "      <td>8634325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23137</td>\n",
       "      <td>1837767</td>\n",
       "      <td>8016698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>38946</td>\n",
       "      <td>2139492</td>\n",
       "      <td>2138910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>27424</td>\n",
       "      <td>644919</td>\n",
       "      <td>8016698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>41524</td>\n",
       "      <td>317415</td>\n",
       "      <td>8016698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>32830</td>\n",
       "      <td>1888876</td>\n",
       "      <td>8634325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>32830</td>\n",
       "      <td>1888876</td>\n",
       "      <td>315470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id   poi_id rec_poi_id\n",
       "0      7435  2149128     310900\n",
       "1      7435  2149128    4400781\n",
       "2      7435  2149128    1837767\n",
       "3     23137  1837767    8634325\n",
       "4     23137  1837767    8016698\n",
       "..      ...      ...        ...\n",
       "699   38946  2139492    2138910\n",
       "700   27424   644919    8016698\n",
       "701   41524   317415    8016698\n",
       "702   32830  1888876    8634325\n",
       "703   32830  1888876     315470\n",
       "\n",
       "[704 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all relevant retrieved instance by merging the relevant and recommended interactions\n",
    "df_retrived_relevant = pd.merge(df_all_relevant, df_all_retrieved, on=['user_id', 'poi_id', 'rec_poi_id'], how='inner')\n",
    "df_retrived_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0eb727",
   "metadata": {},
   "source": [
    "Number of retrived_relevant records\n",
    "\n",
    "algo [1,2,3,4]: 704 \n",
    "\n",
    "algo [2,3,4]: 274 \n",
    "\n",
    "algo [1,3,4]: 612 \n",
    "\n",
    "algo [1,2,4]: 133 \n",
    "\n",
    "algo [1,2,3]: 639  \n",
    "\n",
    "algo [1,2]: 90\n",
    "\n",
    "algo [1,3]: 540 \n",
    "\n",
    "algo [1,4]: 72\n",
    "\n",
    "algo [2,3]: 179 \n",
    "\n",
    "algo [2,4]: 51\n",
    "\n",
    "algo [3,4]: 142 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4eda5dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.409778812572759\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision score\n",
    "relevant_retrieved = df_retrived_relevant.shape[0]\n",
    "all_retrived = df_all_retrieved.shape[0]\n",
    "\n",
    "precision = relevant_retrieved / all_retrived\n",
    "\n",
    "print(f'Precision Score: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b9ea2",
   "metadata": {},
   "source": [
    "Precision Score \n",
    "\n",
    "algo [1,2,3,4]: 0.409778812572759\n",
    "\n",
    "algo [2,3,4]: 0.4990892531876138\n",
    "\n",
    "algo [1,3,4]: 0.6101694915254238\n",
    "\n",
    "algo [1,2,4]: 0.16180048661800486\n",
    "\n",
    "algo [1,2,3]: 0.42206076618229854\n",
    "\n",
    "algo [1,2]: 0.13953488372093023\n",
    "\n",
    "algo [1,3]: 0.6513872135102533\n",
    "\n",
    "algo [1,4]: 0.4235294117647059\n",
    "\n",
    "algo [2,3]: 0.5700636942675159\n",
    "\n",
    "algo [2,4]: 0.2537313432835821\n",
    "\n",
    "algo [3,4]: 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19a28036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.3080962800875274\n"
     ]
    }
   ],
   "source": [
    "# calculate the recall score\n",
    "relevant_retrieved = df_retrived_relevant.shape[0]\n",
    "all_relevant = df_all_relevant.shape[0]\n",
    "\n",
    "recall = relevant_retrieved / all_relevant\n",
    "print(f'Recall Score: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2143746",
   "metadata": {},
   "source": [
    "Recall Score \n",
    "\n",
    "algo [1,2,3,4]: 0.3080962800875274\n",
    "\n",
    "algo [2,3,4]: 0.11991247264770241\n",
    "\n",
    "algo [1,3,4]: 0.26783369803063456\n",
    "\n",
    "algo [1,2,4]: 0.05820568927789934\n",
    "\n",
    "algo [1,2,3]: 0.27964989059080964\n",
    "\n",
    "algo [1,2]: 0.03938730853391685\n",
    "\n",
    "algo [1,3]: 0.2363238512035011\n",
    "\n",
    "algo [1,4]: 0.03150984682713348\n",
    "\n",
    "algo [2,3]: 0.07833698030634573\n",
    "\n",
    "algo [2,4]: 0.022319474835886213\n",
    "\n",
    "algo [3,4]: 0.062144420131291025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "764de80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage Score: 0.6811594202898551\n"
     ]
    }
   ],
   "source": [
    "# calculate the coverage score\n",
    "num_recommended_pois = df_all_retrieved['rec_poi_id'].nunique()\n",
    "num_all_pois = df_pois.shape[0]\n",
    "\n",
    "coverage = num_recommended_pois / num_all_pois\n",
    "print(f'Coverage Score: {coverage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a9c1c",
   "metadata": {},
   "source": [
    "Coverage Score \n",
    "\n",
    "algo [1,2,3,4]: 0.6811594202898551\n",
    "\n",
    "algo [2,3,4]: 0.5217391304347826\n",
    "\n",
    "algo [1,3,4]: 0.42028985507246375\n",
    "\n",
    "algo [1,2,4]: 0.5362318840579711\n",
    "\n",
    "algo [1,2,3]: 0.6666666666666666\n",
    "\n",
    "algo [1,2]: 0.463768115942029\n",
    "\n",
    "algo [1,3]: 0.37681159420289856 \n",
    "\n",
    "algo [1,4]: 0.10144927536231885\n",
    "\n",
    "algo [2,3]: 0.4492753623188406\n",
    "\n",
    "algo [2,4]: 0.15942028985507245\n",
    "\n",
    "algo [3,4]: 0.17391304347826086"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43798c",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "Delete both the GDS in-memory state and the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe3961e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graphName                                                          myGraph\n",
       "database                                                             neo4j\n",
       "databaseLocation                                                     local\n",
       "memoryUsage                                                               \n",
       "sizeInBytes                                                             -1\n",
       "nodeCount                                                            58725\n",
       "relationshipCount                                                   170068\n",
       "configuration            {'relationshipProjection': {'REVIEWED': {'aggr...\n",
       "density                                                           0.000049\n",
       "creationTime                           2024-03-15T07:05:16.639119930+00:00\n",
       "modificationTime                       2024-03-15T07:05:36.532602204+00:00\n",
       "schema                   {'graphProperties': {}, 'nodes': {'User': {'em...\n",
       "schemaWithOrientation    {'graphProperties': {}, 'nodes': {'User': {'em...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove our projection from the GDS graph catalog\n",
    "G.drop()\n",
    "\n",
    "# Remove all the example data from the database\n",
    "# _ = gds.run_cypher(\"MATCH (n) DETACH DELETE n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
